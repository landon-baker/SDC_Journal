# July 24, 2019

## Thoughts:

Today I spent the majority of my time seeding two local database instances with 10 million records. I decided to use Mongo as my no-SQL option and Postgres as the SQL database.

Seeding 10 million records was a bit tricky at first. My plan was to write a csv file and import it to both databases, but the csv file came with some challenges. Ultimately what worked was writing a loop that would generate 1 million records at a time (data was generated using [faker.js](https://github.com/marak/Faker.js/)). I used node's fs module to build up the csv 1 million at a time becuase trying to write all 10 million at once generated a heap out of memory error.

After both databases were seeded, I tried to do some simple timed queries and test the differences between SQL and no-SQL. The seeding itself took approx 2m45s for all 10 million records in the local Mongo instance. The Postgres seeding was significantly faster at 54s using the same csv file. Both Postgres and SQL offered essentially instant lookups when the lookup was performed on the Mongo Object ID or on the primary key in Postgres. Using another unique identifier (not the ones just mentioned) - the uuid - the Postgres lookup took ~1 second on average while the Mongo lookup took ~3-4 seconds for a single record. Finally, a grouped query for a price search took ~7 seconds in Postgres, and it took ~15 seconds for Mongo to return all 106 documents.

## Challenges faced:

trying to batch more than 1 million at a time:

`FATAL ERROR: Ineffective mark-compacts near heap limit Allocation failed - JavaScript heap out of memory`

## Results/Takeaways:

The aforementioned queries are admittedly pretty surface-level tests, but they seem to indicate that Postgres is measurably more performant than Mongo for simple queries targeted at this large of a dataset.
